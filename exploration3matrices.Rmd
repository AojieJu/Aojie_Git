---
title: 'Exploration 3: Matrices make life easier.'
author: "SÃ¸ren Warland, Ikromjon Tuhtasunov, Matt Mettler, Aojie Ju"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    number_sections: true
    fig_caption: yes
    latex_engine: xelatex
    citation_package: biblatex
    keep_tex: true
    fig_height: 8
    fig_width: 8
graphics: yes
geometry: "left=1.25in,right=1.25in,top=1in,bottom=1in"
fontsize: 11pt
bibliography: classbib.bib
biblio-style: authoryear-comp
---

<!-- Make this document using library(rmarkdown); render("exploration2.Rmd") -->
\input{mytexsymbols}

```{r setup, echo=FALSE, results=FALSE, include=FALSE, cache=FALSE}
library(here)
library(tidyverse)
library(readstata13)
library(estimatr)
source(here::here("rmd_setup.R"))
```


Imagine that you receive a WhatsApp from old friend. He is from one of the new
political analytics firms that started to grow awhile back during the first
Obama campaign in the USA and he has a prediction problem. He has a small
dataset of 8 cities and would like to predict current voting turnout in those
cities based on a complex model. He says, "My last analyst provided the
following code to fit my model but then stopped. I think that the best model of
voting turnout uses median household income, median age, racial diversity
(here, percent african american), and the number of candidates running for this
city council office. I told the analyst this model and he provided the
following code. Can you help me?"


```{r getthedata}
news.df <- read.csv("news.df.csv")
## Uncomment out the following line for your own use to download the csv from the web
## news.df<-read.csv("http://jakebowers.org/Data/news.df.csv")
news.df$sF <- factor(news.df$s)
```

"I really don't understand the following `lsCriterion` function. Can you write out the criterion using math and explain it to me in plain language? I'm always especially interested in understanding **why** these stats types are doing this stuff, and I'm so grateful that you can explain it simply and plainly to me."


```{r}

lsCriterion <- function(b, y, X) {
  yhat <- b[1] * X[, 1] + b[2] * X[, 2] + b[3] * X[, 3] + b[4] * X[, 4] + b[5] * X[, 5]
  ehat <- y - yhat
  thessr <- sum(ehat^2)
  return(thessr)
}

X <- as.matrix(cbind(constant = 1, news.df[, c("medhhi1000", "medage", "blkpct", "cands")]))
y <- news.df$rpre

#This function is setting up a linear equation for a set of variables. The resulting equation will be a model of voter turnout given several variables. The variable b is the the expected coefficients, y is the observed value, and X are the various variables that will be tested in the model. After obtaining a matrix of values to test, the function then subtracts the predicted and observed values. It then squares these values to get the square of the residuals. A good model will return a small value for the sum of the squared residuals.

```

"He said to 'try some different vectors' and I think that this meant that I was to guess about the values for the coefficients in the model and that, after trying a bunch, I would choose the vector that I liked best. So, for example, I tried a model with all zeros:"

```{r}
lsCriterion(c(0, 0, 0, 0, 0, 0), y = y, X = X)
```

And then tried to see a bunch of other models.

```{r}
lsCriterion(c(1, 0, 4, 0, 0, 0), y = y, X = X) ## hmm..ok... can I do better?
set.seed(12345)
lsCriterion(runif(5, -100, 100), y = y, X = X) ## bad
```

"After trying a bunch of models, however, I started to get tired. Can you help me do this faster? I asked the analyst (who is really a construction engineer and not a statistician) if I could use a loop but he said it would be better to 'try to optimize the objective function' and this is as far as I got."



```{r}
lsSolution <- optim(
  fn = lsCriterion, par = c(0, 0, 0, 0, 0), X = X, y = y,
  method = "BFGS", control = list(trace  = 1, REPORT = 1)
)
```

"Is this the best solution? How well does this model predict the actual outcome (the variable ` r `) (this model uses baseline turnout or `rpre`). Can you give me some quantitative measure of how well our model predicted the outcome? I think that you can use the code from the `lsCriterion` function to develop predictions and compare our predictions to the actual turnout observed (the variable ` r`), right?"

#The optim function in R finds the model that minimizes the sum of the squares. After a few trial runs, the model that it found that minimized the sum of the squares still found that their sum was equal to 872.5, which seems far too high to say that the model was a good fit.


"Now, I wanted to add another variable to see how well that new model fit. For example, maybe `blkpct` has a curvilinear relationship with the outcome. I complained to the analyst that I would have to re-write the function every time that I had a new model. So, the analyst said, 'You don't have to do that, just use matrices.' and he sent this:"

```{r}
bVector <- solve(t(X) %*% X) %*% t(X) %*% y
yhat <- X %*% bVector
ehat <- y - yhat
summary(ehat)
```

"Now, I'm very impressed at the speed and conciseness of this! I mean, he got the same vector of coefficients in like 1/1000 the time that it took me to search for a solution --- even using a fast optimizer. Also, he got the predictions very quickly too! But I'm confused about how this works. I understand the idea of proposing different values for the different coefficients and then asking how well they do --- in a sum of squared error sense. But the three lines that create `bVector` and `yhat` and even `ehat` are a mystery and I worry that they are not actually comparing my predictions of past turnout to observed future turnout (`rpre` versus ` r`). Maybe you can help? I asked the analyst to provide a little guidance to help you get practice with these ideas."

#The code above puts the vectors for the regression line in a matrix form and then performs multiplication in order to get the predicted values. Then, the differences of the predicted and observed values are calculated.


So, you need to be able to explain what is happening when we tell R to do least squares with the matrix $\bX$ and
vector $\by$ via $(\bX^{T}\bX)^{-1}\bX^{T}\by$ using the command
`solve(t(X)%*%X)%*%(t(X)%*%y)`. Where
`t(X)`$\equiv \bX^{T}$ (meaning the transpose of $\bX$) and
`solve(X)`$\equiv \bX^{-1}$ (meaning the inverse of $\bX$).

Here are some steps you might take to produce this explanation.

1.  First, let's create a $\bX$ matrix using the newspapers data to
  do a regression of the form `baseline.turnoutx~income+median age` (where
  baseline.turnout is `rpre` and income is `medhhi1000` and median age of the city is `medage`). Here is
  how one might do this in R for both $\bX$ and $\by$ (where,
  \textbf{bold} represents matrices or vectors):

```{r setupXandy, results='hide'}
X <- as.matrix(cbind(1, news.df$medhhi1000, news.df$medage)) # "bind" together columns from the data and an intercept
y <- matrix(news.df$rpre, ncol = 1) # not strictly necessary, y<-news.df$rpre would also work

X
y

# Structure of the objects
str(X)
str(y)

# Look at the dimensions of the objects: number rows by number columns
dim(X)
dim(y)
```
   - Explain how we created the $\bX$ matrix \emph{Hint:} The column of 1s has to do with the intercept or constant term.
   - What do the columns of $\bX$ represent?
   - What do the rows represent?
   
#The matrix has three columns. The first column, made up of 1's, is a baseline intercept. The next consists of th median income for the cities, and the final column is their median age. Each of the rows represents an observation from each of the cities.

#Y, on the other hand, is the 8 x 1 matrix of predicted turnout for each city. Solving the matrix multiplication will result in the intercept and coefficients or the variables.
  

2. First, addition and subtraction: Try each of the following lines of math in R and explain to yourself (or your colleagues) what happened, and what this means about matrix math. I did the first one as an example.

  Explain what is happening with each of the following
```{r addition,echo=TRUE,results='markup'}
X + 2
#It represents that two (scalar) plus each entry. 
```

"When you add a single number (aka a scalar) to a matrix, the scalar is added to each entry in the matrix."

Notice: If we didn't have matrix math, here is what we'd have to do to
add a scalar to a matrix
```{r add2,echo=TRUE,tidy=TRUE}
Xplus2 <- matrix(NA, nrow = 8, ncol = 3) # explain
for (row.entry in 1:8) { # explain
  for (col.entry in 1:3) { # explain
    Xplus2[row.entry, col.entry] <- X[row.entry, col.entry] + 2 # explain
  }
}

(X + 2) == Xplus2 #  explain
# An easier check on whether two objects are the same (except for
# names and such)
all.equal((X + 2), Xplus2, check.attributes = FALSE)

X - 2 # Explain

#The equation above contains two for loops, one to cycle through the columns and the other to cycle through the rows. For every value that the for loop comes across, 2 is added. Below, Boolean logic is used to see if adding 2 to every entry using matrix algebra is the same as the code above that added 2 to the values one at a time. Because all the values returned are TRUE, the two matrices are the same.
```

```{r vecmatadd,echo=TRUE}
twovec <- matrix(c(2, 2, 2), nrow = 1, ncol = 3) # explain
twomat <- matrix(2, nrow = 8, ncol = 3) # explain

#Similar to how we can add a scalar to a matrix, we can also subtract one. Here, 2 is subtracted from all values of the matrix using matrix algebra.
```


You'll see some errors appear below. Your job is to explain why R
failed or made an error. I had to surround these lines in `try()`
to prevent R from stopping at the error.

```{r vecmatadd2,echo=TRUE,message=TRUE, warning=TRUE, error=TRUE}
try(X + twovec) # explain
try(X + t(twovec)) # Here, you need to explain what t(twovec) does.

#Here, instead of specifying a list to be added to the matrix, only a single value is specified. As a result, R populates all 8 rows and 3 columns with the same value.
#Matrices can only be added if they have the same dimensions. Twovec has a 1 x 3 dimension, but X has a size of 8 x 3.
#The function "t" in R transposes a matrix: it changes all of the columns to rows and all the rows to columns. A transposition of twovec would result in a matrix with dimensions 3 x 8. Addition still will not work between these two matrices because they are not of the same dimensions. However, multiplying the matrices would result in a valid 1 x 8 array.
```


```{r matmat,echo=TRUE,results='hide',tidy.opts=list(keep.comment=TRUE)}
X + twomat # explain
#Here Boolean logic is used to see if all the values of X after twomat was added via matrix algebra is equal to simply adding 2 to each value of X. Since the output array contains all TRUE, the two matrices are equal.

(X + twomat) == (X + 2) # explain
#The all.equal function has a single value as an output, TRUE if the matrices are equal in every value and FALSE if they are not. Here we see that the output yields true.

all.equal((X + twomat), (X + 2), check.attributes = FALSE) # explain
all.equal((X + twomat), (twomat + X), check.attributes = FALSE) # explain
#Here we see the commutative property of matrices, where A + B = B + A. The TRUE output of R shows this property. 
```


```{r self,results='hide',tidy.opts=list(keep.comment=TRUE)}
X + X # why does this work?
#These two matrices (essentially the same matrix) are of the same dimensions, so they can be added.
```

3. Second, multiplication. Notice that the symbols for scalar
  multiplication and matrix multiplication are not the same. Try each
  of the following lines of math in R and explain to yourself (or
  your colleagues) what happened, and what this means about matrix
  math.

```{r multiplication,results='hide',tidy.opts=list(keep.comment=FALSE)}

X * 2 # Explain
#Each value of the matrix X is multiplied by 2.
all.equal((X * 2), (2 * X)) # explain
#This line of code shows that multiplying the matrix by 2 will produce the same result as multiplying 2 by each value of the matrx

X^2 #  # explain
#Each entry of the matrix X is squared.

X^.5 # # explain
#Each entry of the matrix X is raised to the 1/2 power.

sqrt(X) # # explain
#Each entry of the matrix X has its square root taken. We get the same result as X^.5.
```

Now, let's get a vector of coefficients to make matrix math link even more
tightly with what we've already done fitting models to data:

```{r}
b <- solve(t(X) %*% X) %*% t(X) %*% y # explain
#X contains our known values and the generic intercept in the first column. y is the matrix of predicted values. The transposition of X results in a 3 x 8 matrix, so the transposed version can then be multiplied by Y, a matrix of dimensions 8 x 1. The resulting output is a matrix of dimensions 3 x 1.
dim(b) # explain
#The equation above means that a 3 x 8 matrix is multiplied by an 8 x 1 matrix to get the solution. X has to be transposed in order for the math to work.
```

Now, let's do matrix multiplication the tedious way.  What does this
function tell us about the rule for doing matrix multiplication? Please explain the lines and the function here.

```{r tediousmult,results='hide',tidy=TRUE,tidy.opts=list(keep.comment=FALSE)}

X.times.b <- matrix(NA, nrow = 8, ncol = 1) # Initialize a results matrix

for (row.entry in 1:8) { # explain
  temp <- vector(length = 3) # initialize the temp vector
  for (col.entry in 1:3) { # explain
    temp[col.entry] <- X[row.entry, col.entry] * b[col.entry, ] # explain
  }
  X.times.b[row.entry, ] <- sum(temp)
}
X.times.b
#The first row initializes a matrix and populates it with NA's to begin with. This matrix has dimensions of 8 x 1. The second row specifies that a certain operation will be done on all of the rows of the matrix. The next row creates a temporary vector for calculated values to be stored. 
#After that, another for loop specifies that all three columns of the temporary vector should be populated with the product of the value in matrix X and the value i matrix b. Then, all of these values are summed so that there is only one column in the final matrix with dimensions of 8 x 1.
```

Now, doing part of it by hand: Please explain the lines of code.
```{r byhand,results='hide',tidy.opts=list(keep.comment=TRUE)}

(X[1, 1] * b[1]) + (X[1, 2] * b[2]) + (X[1, 3] * b[3])

(X[2, 1] * b[1]) + (X[2, 2] * b[2]) + (X[2, 3] * b[3])

(X[3, 1] * b[1]) + (X[3, 2] * b[2]) + (X[3, 3] * b[3])
## etc.... for each row in X
```

And now a little faster (multiplying vectors rather than scalars and
summing): You can break matrix multiplication into separate vector
multiplication tasks since vector multiplication also goes
sum-of-row-times-column. Please explain all of this code.
```{r vectorized,results='hide',tidy.opts=list(keep.comment=FALSE)}
X[1, ] %*% b # Explain
X[2, ] %*% b # Explain

#These two lines take one row (consisting of three values) at a time from X and then multiply all of these values by b. %*% is the matrix multiplication operator in R. The result is one value since the three values of each row of x are multiplied by the three values of b and then added to get one number.
```

And doing it very fast: This is direct matrix multiplication. So nice
and clean compared to the previous! Don't we love matrix multiplication?
```{r fast,results='hide'}
X %*% b
```

How does `fitted(thelm)` relate to `X %*% b`? What is `%*%` in  `X %*% b` (often written $\bX \bb$ or $\bX \hat{\bbeta}$)?


```{r lmstuff}
thelm <- lm(rpre ~ medhhi1000 + medage, data = news.df)
fitted(thelm)

#The 1 x 8 array that fitted(thelm) returns is the predicted turnout values matrix, rather than the observed array that would be used as an operand in matrix multiplication in order to get the 1 x 3 matrix of coefficients. The %*% operator is the matrix multiplication operator in R.
```


4. How would you use matrix addition/subtraction to get the residuals once
  you had $\bX \bb$ (aka `X %*% b`)?
  
You would subtract the matrix generated by the fitted(thelm) function above from the matrix Y of predicted values in order to get the residuals.

5. Now, let's meet another important matrix:

```{r xtx,error=TRUE,warning=TRUE,message=TRUE}
# Now another important matrix:
try(X %*% X) # why doesn't this work.

#The code above multiplies an 8 x 3 and 8 x 3 matrix together. Multiplication only works when the number of columns in the first matrix is the same as the number of rows on the second.
```



```{r xtx2,results='hide'}
t(X) # What is this?
#This function transposes X so that it is now a 3 x 8 matrix. All its columns have become rows and vice versa. As a result, multiplying by an 8 x 3 matrix will now yield a 3 x 3 matrix.

XtX <- t(X) %*% X # What is XtX

XtX

#It equals to the trasposition of X multiplies x.
```

To make our lives easier, let's mean deviate or center or align all of the variables
(set it up so that all of them have mean=0). Now the `XtX`
matrix will be easier to understand:


```{r}
colmeansvec <- colMeans(X) # What does this do?
#This vector contains 3 values, one for each of the columns in X. These values correspond to the mean of these three columns.

colmeansmat <- matrix(rep(colmeansvec, 8), ncol = 3, byrow = TRUE)
# Fill a matrix with those means: repeat each row 8 times and stack them.
```

Why would we want a matrix like the following?
```{r meancent,echo=TRUE,results='hide'}
X - colmeansmat
# Subtract the column means from each element of X
# What is happening here?
#We can now see which values are above and below the mean. All of the values in X have now been centered around a mean of 0.

# Or here?
t(apply(X, 1, function(x) {
  x - colmeansvec
}))
# And here is another way to do it:
sweep(X, 2, colmeansvec) # See the help page for sweep
#The function sweep generates an output matrix based on an input matrix and a statistical command, in this case the mean of the columns to be taken out of the values.
```

Please explain this code:

```{r xtx3,results='hide'}
X.md <- X - colmeansmat
y.md <- y - mean(y)
XtX.md <- t(X.md) %*% X.md
XtX.md
#Covariance is a concept that relies on the standard deviation. Calculating the covariance involves subtracting all values of x from the mean of x and all values of y from the mean of y. Then, these values are multiplied together, as could be done in matrix multiplication. The first two lines of code could be used in such an operation.

#When the matrix made by subtracting all values of X by the column means is created and then transposed to be multiplied by itself, the result is the sums of the squares. This value could be turned into the variance by dividing by (n-1).
```

Explain what each entry in `XtX.md` is: if you can, relate those numbers to
quantities like variances, covariances, sums (of squares or not), sample sizes, do so.

#"XtX.md" dividing by (n-1) is the covariance matrice, in which each entry dividing by (n-1) is the covariance of two of the variables in the regression. 

Here another representation of `XtX` that might help you explain and reproduce
the entries above where $x_{1i}$ and $x_{2i}$ represent the two covariates in
our prediction model.

$$\bX^{T}\bX=\begin{bmatrix} n & \sum_{i = 1}^{n}{x_{1i}} & \sum_{i =
    1}^{n}{x_{2i}} \cr \sum_{i = 1}^{n}{x_{1i}} & \sum_{i = 1}^{n}
  {{x_{1i}}}^2 & \sum_{i = 1}^{n}{x_{1i}}{x_{2i}} \\ \sum_{i =
    1}^{n}{x_{2i}} & \sum_{i = 1}^{n} {x_{1i}}{x_{2i}} & \sum_{i =
    1}^{n}{{x_{2i}}}^2 \end{bmatrix}$$

Try some of the following commands to get some other help in understanding what XtX is:

```{r XtX,results='hide',tidy.opts=list(keep.comment=FALSE)}
sum(X.md[, 1])

sum(X.md[, 2]^2)
sum((X.md[, 2] - mean(X.md[, 2]))^2) #
sum((X.md[, 2] - mean(X.md[, 2]))^2) / (8 - 1) #
var(X.md[, 2]) # Another way to get variance of z

sum(X.md[, 2] * X.md[, 3]) # why not use %*%? (ans: we want the cross-product for the covariance)
sum(X.md[, 2] * X.md[, 3]) / (8 - 1) #

cov(X.md) # see the help file on cov(): The variance-covariance matrix

XtX.md / (8 - 1) # What is this?
#We are dividing by n-1 in order to get the variance. The values in the resulting matrix are the covariances for the three dimensions of the model.
```

What about $\bX^{T} \by$? Explain the entries in `Xty.md`. Explain the code.

```{r Xty,results='hide',tidy.opts=list(keep.comment=FALSE)}
t(X.md)

y.md

Xty.md <- t(X.md) %*% y.md
Xty.md # What is this?

cov(cbind(X.md, y.md)) #

Xty.md / 7 #
#'Xty.md' dividing by (n-1) equals to the variances of y. 
```

The following is a verbal formula for a covariance:
deviations of x from its mean times deviations of y from its mean
divided by n-1 (roughly the average of the product of the deviations)
```{r xty2,results='hide'}
sum((X[, 2] - mean(X[, 2])) * (y - mean(y)))
sum((X[, 2] - mean(X[, 2])) * (y - mean(y))) / (8 - 1)

# Same as above because we've removed the means from X.md and y.md
sum((X.md[, 2]) * (y.md)) / (8 - 1)

Xty <- t(X) %*% y
Xty
Xty / (8 - 1)
```

4. And finally division: Try each of the following lines of math in
  R and explain to yourself (or your colleagues) what happened
  (ideally relate what happened to ideas about variances, covariances,
  sums, etc..)

```{r div,results='hide',tidy.opts=list(keep.comment=FALSE)}

X / 2 # Explain
#Each of the values in the matrix was divided by 2

1 / X # Explain
#Same as above: 1/X==(X^(-1)). It times X and we can get an identity matrix.

X^(-1) # Same as above: 1/X==(X^(-1))

1 / X == (X^(-1))

# Now matrix inversion using solve()
try(solve(X)) # Doesn't work --- why?
#When solve is used in R given only a single matrix, it finds the inverse of the matrix. However, to have an inverse the matrix must be square. The 8 x 3 matrix X is not square.

solve(XtX) # This works, why?
#XtX is a 3 x 3 matrix, so it has a valid inverse.

dim(XtX)
dim(Xty)


solve(XtX) %*% Xty # WOW! AMAZING! WONDERFUL! (what is this?)
#Here we see that the inverse of XtX is multiplied by Xty, which is the transpose of X multiplied by y. The result is the intercept and the coefficients for a regression equation.

try(solve(XtX.md)) #  What is the problem?
#The transpose of X does not provide useful results, perhaps because of the intercepts centered at 0. Selecting only columns 2 and 3 ensures that we do not include the intercept values in the final calculation.

XtX.md <- t(X.md[, 2:3]) %*% X.md[, 2:3] # Hmm? What is happening?
#We just leave out the row and column without any non-zero entries.

try(solve(XtX.md))

Xty.md <- t(X.md[, 2:3]) %*% y.md

solve(XtX.md) %*% Xty.md
```

```{r div2}
# Notice that this is not the same as
(1 / XtX) %*% Xty # Matrix inversion is different from scalar division

# How does this help us?
lm(I(rpre - mean(rpre)) ~ I(medhhi1000 - mean(medhhi1000)) + I(medage - mean(medage)) - 1, data = news.df)
#Here we have a linear model that also centers the values around zero so that it can better be compared to the matrices whose values were also centered around 0.

# or rpre
lm(scale(rpre, scale = FALSE) ~ scale(medhhi1000, scale = FALSE) + scale(medage, scale = FALSE) - 1, data = news.df)
# or
lm(y.md ~ X.md[, 2] + X.md[, 3] - 1)
coef(lm(rpre ~ medhhi1000 + medage, data = news.df))
```
6. So, the vector of least squares coefficients is the result of
  dividing what kind of matrix by what kind of matrix? (what kind of
  information by what kind of information)? \emph{Hint:} This
  perspective of "accounting for covariation" is another valid way
  to think about what least squares is doing [in addition to smoothing
  conditional means]. They are mathematically equivalent.

Why should covariances divided by variances amount to differences of
means, let alone adjusted differences of means?

#The vector of least squares coefficients is calculated by multiplying a transposed matrix by a matrix in its original form. The first matrix contains all of the values of the variables involved centered aroun zero.The second matrix contains the mean of the y values (predicted outcomes) subtracted from the column 


Here are some definitions of covariance and variance:

$$\cov(X,Y)=\frac{\sum_{i}^{n}(X_i - \bar{X})(Y_i - \bar{Y})}{n-1} $$
$$\var(X)=\cov(X,X)=\frac{\sum_{i}^{n}(X_i - \bar{X})(X_i - \bar{X})}{n-1}=\frac{\sum_{i}^{n}(X_i - \bar{X})^2}{n-1}
$$

So, first,

$$\frac{\cov(X,Y)}{\var(X)}=\frac{\sum_{i}^{n}(X_i -
  \bar{X})(Y_i - \bar{Y})}{\sum_{i}^{n}(X_i - \bar{X})^2}$$

because
the (n-1) cancels out. (Thus, we had to divide $X^{T}X$ and $X^{T}y$
by n-1 in the sections above to get the analogous
covariances/variances). So, this is the bivariate case with
$y=\beta_0+\beta_1 x_1$. What about $y=\beta_0+\beta_1 x_1+ \beta_2 x_2$?

This becomes notationally messy fast.  Already,
however, you can get a sense for the idea of deviations from the mean
being a key ingredient in these calculations.

7.  Why might $\bX \boldsymbol{\beta}$ be useful? Let's get back to the question of prediction. So far we have the a model that predicts future turnout with the following squared error. Please explain the following code.

```{r}

X <- as.matrix(cbind(constant = 1, news.df[, c("medhhi1000", "medage", "blkpct", "cands")]))
y <- news.df$rpre
bVector <- solve(t(X) %*% X) %*% t(X) %*% y
yhat <- X %*% bVector
errors <- news.df$r - yhat
summary(errors)
mseOLS <- mean(errors^2)
#This code creates a matrix of the relevant predictor variables for all 8 observations, and then another matrix, Y, is created with the predicted tunrout values. The transpose of X multiplied by X is then multiplied by the transpose of X multiplied by Y. Predicted Y values are obtained by multiplying this resulting vector by X to get a 1 x 8 matrix. Then, the observed minus the predicted scores is used to calculate the residuals. The residuals are not exactly centered on 0, suggesting that the model isn't a good fit.
```

Now, we suspect that we could do better than this from @james2013introduction. Here is an example using the lasso. What do you think? Did we do a better job than OLS in this small dataset? What is going on in this code? Please explain the code.

This code avoids letting the imntercept have undue influence over the model. It calculates predicted values with the X and b matrices. A given lambda is multiplied by the intercept calculation and the sum of the squares. The goal is to decrease the coefficients.

```{r}

lassoCriterion <- function(lambda, b, y, X) {
  ## Assumes that the first column of X is all 1s
  yhat <- X %*% b
  ehat <- y - yhat
  l1.penalty <- sum(abs(b[-1])) ## no penalty on intercept
  thessr <- sum(ehat^2)
  lassocrit <- thessr + lambda * l1.penalty
  return(lassocrit)
}

lassoCriterion(lambda = .5, b = rep(0, ncol(X)), y = y, X = X)
lassoSol <- optim(
  par = c(0, 0, 0, 0, 0),
  fn = lassoCriterion,
  method = "BFGS",
  X = X,
  y = y,
  lambda = .5,
  control = list(trace = 1, REPORT = 1)
)

yhatL1 <- X %*% lassoSol$par
errorsL1 <- news.df$r - yhatL1
mseL1 <- mean(errorsL1^2)

## Now see if we can find the best value of lambda
nlambdas <- 100
somelambdas <- seq(.001, 100, length = nlambdas)

## A function to get lasso criterion coefs and MSE

getlassoMSE <- function(lambda, X, y) {
  lassoSol <- optim(
    par = rep(0, ncol(X)),
    fn = lassoCriterion,
    method = "BFGS",
    X = X,
    y = y,
    lambda = lambda
  )
  # ,control=list(trace=1,REPORT=1))

  yhatL1 <- X %*% lassoSol$par
  errorsL1 <- news.df$r - yhatL1
  mseL1 <- mean(errorsL1^2)
  return(c(par = lassoSol$par, mse = mseL1, lambda = lambda))
}

results <- sapply(somelambdas, function(l) {
  getlassoMSE(l, X = X, y = y)
})
## apply(results,1,summary)
min(results["mse", ]) > mseOLS
results["lambda", results["mse", ] == min(results["mse", ])]
```

```{r, eval=FALSE}
## To do this even faster:
library(glmnet)
lassoFits <- glmnet(x = X[, -1], y = y, alpha = .5) ## using an elastic net fit rather than strict lasso
par(mar = c(6, 2, 3, 1), mgp = c(1.5, .5, 0))
plot(lassoFits, xvar = "lambda", label = TRUE)
## This next line add the raw lambdas since the default plot shows only the log transformed lambdas
axis(1, line = 2, at = log(lassoFits$lambda), labels = round(lassoFits$lambda, 3))
abline(h = 0, col = "gray", lwd = .5)
```

```{r eval=FALSE}

getMSEs <- function(B, X, obsy) {
  yhats <- apply(B, 2, function(b) {
    X %*% b
  })
  ehats <- apply(yhats, 2, function(y) {
    obsy - y
  })
  apply(ehats, 2, function(e) {
    mean(e^2)
  })
}

getMSEs(B = coef(lassoFits), X = X, obsy = y)
```

Hmmm.... should the MSE always just go down? I wonder what @james2013introduction has to say about this.^[I suspect that @james2013introduction would recommend cross-validation --- but we only have 8 observations here, so that would be difficult.]

If the model is complex, it is possible that the best fit will involve MSE increasing rather than decreasing.

Now, more benefits of penalized models. Imagine this model:

```{r}
newmodel <- rpre ~ blkpct * medhhi1000 * medage * cands
lm4 <- lm(newmodel, news.df)
X <- model.matrix(newmodel, data = news.df)

try(b <- solve(t(X) %*% X) %*% X %*% y)

lsCriterion2 <- function(b, y, X) {
  yhat <- X %*% b
  ehat <- y - yhat
  thessr <- sum(ehat^2)
  return(thessr)
}

lsSolution1 <- optim(fn = lsCriterion2, par = rep(0, ncol(X)), X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
lsSolution1

lsSolution2 <- optim(fn = lsCriterion2, par = rep(100, ncol(X)), X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
lsSolution2

lsSolution3 <- optim(fn = lsCriterion2, par = rep(-100, ncol(X)), X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
lsSolution3

cbind(lsSolution1$par, lsSolution2$par, lsSolution3$par)

## Notice that we are not standardizing the columns of X here. Should do that for real use.
ridgeCriterion <- function(lambda, b, y, X) {
  ## Assumes that the first column of X is all 1s
  yhat <- X %*% b
  ehat <- y - yhat
  l2.penalty <- sum(b[-1]^2) ## no penalty on intercept
  thessr <- sum(ehat^2)
  lassocrit <- thessr + lambda * l2.penalty
  return(lassocrit)
}


ridgeSolution1 <- optim(fn = ridgeCriterion, par = rep(0, ncol(X)), lambda = .5, X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
ridgeSolution1

ridgeSolution2 <- optim(fn = ridgeCriterion, par = rep(100, ncol(X)), lambda = .5, X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
ridgeSolution2

ridgeSolution3 <- optim(fn = ridgeCriterion, par = rep(-100, ncol(X)), lambda = .5, X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
ridgeSolution3

cbind(ridgeSolution1$par, ridgeSolution2$par, ridgeSolution3$par)

## A sketch of Cross-validation to choose lambda: here using 3-fold because of the small dataset
cvfn <- function() {
  testids <- sample(1:nrow(X), nrow(X) / 3)
  trainingids <- (1:nrow(X))[-testids]
  ## Fit
  ## Predict yhat for testids
  ## MSE for y_test versus yhat_test
}

## Average of the MSE across folds is CV MSE
```


# References



